{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "49a929ca-68db-4616-9cb7-4bf89054b4c8",
   "metadata": {},
   "source": [
    "### Task 7 Internship feedback sentiment analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "724a7ad2-bc76-45cf-b8e4-b30cb9d8c420",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 1: Install Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4696dc67-3484-4378-a5c6-e72881afd542",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6af45856-fede-44d6-85da-94487f5fb476",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b50d3419-0d1f-4d28-aa61-e9f495d3e0a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"internship_feedback_dataset_large.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fb8eda88-7767-4b57-a841-e794dbffe8c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Shape: (2500, 7)\n",
      "  Feedback_ID        Date Intern_ID      Department       Source  \\\n",
      "0    FB000001  2025-04-01  INT00197    Software Dev       Survey   \n",
      "1    FB000002  2024-11-01  INT00219  Data Analytics  Google Form   \n",
      "2    FB000003  2025-08-01  INT00045           Sales  Google Form   \n",
      "3    FB000004  2025-03-01  INT00039       Marketing        Email   \n",
      "4    FB000005  2025-01-01  INT00093       Marketing  Google Form   \n",
      "\n",
      "                                      Feedback_Text True_Label  \n",
      "0     Tasks were boring and not related to my role.   Negative  \n",
      "1  Workload was normal, nothing unusual. Thank you.    Neutral  \n",
      "2       Poor communication and management issues. ðŸ˜ž   Negative  \n",
      "3  Great learning experience and supportive mentor.   Positive  \n",
      "4    The team was very helpful and I learned a lot.   Positive  \n"
     ]
    }
   ],
   "source": [
    "print(\"Dataset Shape:\", df.shape)\n",
    "print(df.head(5))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "92801782-4326-4e23-a56c-04be3ffd5198",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 3: Clean Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "98604beb-1526-4e9c-98b0-a21765c199dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Cleaned Text Example:\n",
      "                                      Feedback_Text  \\\n",
      "0     Tasks were boring and not related to my role.   \n",
      "1  Workload was normal, nothing unusual. Thank you.   \n",
      "2       Poor communication and management issues. ðŸ˜ž   \n",
      "\n",
      "                                      Clean_Text  \n",
      "0   tasks were boring and not related to my role  \n",
      "1  workload was normal nothing unusual thank you  \n",
      "2     poor communication and management issues ðŸ˜ž  \n"
     ]
    }
   ],
   "source": [
    "def clean_text(text):\n",
    "    text = str(text).lower()\n",
    "    text = re.sub(r\"http\\S+|www\\S+\", \"\", text)   # remove links\n",
    "    text = text.translate(str.maketrans(\"\", \"\", string.punctuation))  # remove punctuation\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "    return text\n",
    "\n",
    "df[\"Clean_Text\"] = df[\"Feedback_Text\"].apply(clean_text)\n",
    "\n",
    "print(\"\\n Cleaned Text Example:\")\n",
    "print(df[[\"Feedback_Text\",\"Clean_Text\"]].head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "21245e85-deac-4065-ba48-8cf863433e32",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 4: Sentiment Scoring + Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9d2f3d60-528b-4845-a1de-ca29305223e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Sentiment Analysis Sample Results:\n",
      "                                    Feedback_Text  Sentiment_Score Predicted_Sentiment\n",
      "    Tasks were boring and not related to my role.        -0.222222            Negative\n",
      " Workload was normal, nothing unusual. Thank you.        -0.142857            Negative\n",
      "      Poor communication and management issues. ðŸ˜ž        -0.333333            Negative\n",
      " Great learning experience and supportive mentor.         0.500000            Positive\n",
      "   The team was very helpful and I learned a lot.         0.200000            Positive\n",
      "Very satisfied with management and communication.         0.166667            Positive\n",
      "   The team was very helpful and I learned a lot.         0.200000            Positive\n",
      "                  Training sessions were average.         0.000000             Neutral\n",
      "                      Nothing major to highlight.         0.000000             Neutral\n",
      " Great learning experience and supportive mentor.         0.500000            Positive\n"
     ]
    }
   ],
   "source": [
    "positive_words = set([\n",
    "    \"great\",\"good\",\"excellent\",\"amazing\",\"helpful\",\"supportive\",\"enjoyed\",\"enjoy\",\n",
    "    \"satisfied\",\"friendly\",\"productive\",\"timely\",\"constructive\",\"learned\",\"learning\",\n",
    "    \"clear\",\"meaningful\",\"happy\",\"improved\",\"guidance\",\"exposure\"\n",
    "])\n",
    "\n",
    "negative_words = set([\n",
    "    \"lack\",\"unclear\",\"workload\",\"unrealistic\",\"stressful\",\"disorganized\",\n",
    "    \"boring\",\"poor\",\"issues\",\"late\",\"insufficient\",\"confusing\",\"disappointing\",\n",
    "    \"valued\",\"deadline\",\"deadlines\",\"not\"\n",
    "])\n",
    "\n",
    "def sentiment_score(text):\n",
    "    tokens = clean_text(text).split()\n",
    "    pos = sum(1 for t in tokens if t in positive_words)\n",
    "    neg = sum(1 for t in tokens if t in negative_words)\n",
    "    return (pos - neg) / (len(tokens) + 1e-6)\n",
    "\n",
    "def classify(score):\n",
    "    if score > 0.02:\n",
    "        return \"Positive\"\n",
    "    elif score < -0.02:\n",
    "        return \"Negative\"\n",
    "    else:\n",
    "        return \"Neutral\"\n",
    "\n",
    "df[\"Sentiment_Score\"] = df[\"Feedback_Text\"].apply(sentiment_score)\n",
    "df[\"Predicted_Sentiment\"] = df[\"Sentiment_Score\"].apply(classify)\n",
    "\n",
    "print(\"\\n Sentiment Analysis Sample Results:\")\n",
    "print(df[[\"Feedback_Text\",\"Sentiment_Score\",\"Predicted_Sentiment\"]].head(10).to_string(index=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5d47906e-6681-4f2e-97c2-80b73ebf50b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 5: Show Sentiment Counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "41047142-3493-4180-ab03-5b49809d0c00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Sentiment Count Summary:\n",
      "Predicted_Sentiment\n",
      "Positive    1499\n",
      "Neutral      554\n",
      "Negative     447\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n Sentiment Count Summary:\")\n",
    "print(df[\"Predicted_Sentiment\"].value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "bc540d13-dcde-4bac-bc5d-b69fc7337516",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 6: Confusion Matrix + Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1e690319-4f2e-4361-b542-472fc953f47b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Confusion Matrix (True vs Predicted):\n",
      "Predicted_Sentiment  Negative  Neutral  Positive\n",
      "True_Label                                      \n",
      "Negative                  344       55        15\n",
      "Neutral                    83      461       167\n",
      "Positive                   20       38      1317\n",
      "\n",
      " Model Accuracy: 84.88%\n"
     ]
    }
   ],
   "source": [
    "accuracy = (df[\"Predicted_Sentiment\"] == df[\"True_Label\"]).mean()\n",
    "conf_matrix = pd.crosstab(df[\"True_Label\"], df[\"Predicted_Sentiment\"])\n",
    "\n",
    "print(\"\\n Confusion Matrix (True vs Predicted):\")\n",
    "print(conf_matrix)\n",
    "\n",
    "print(f\"\\n Model Accuracy: {accuracy:.2%}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b4a6613d-35f7-4b56-9763-e52f95e40311",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 7: Save Final Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "6d7083d9-dc1a-4381-9657-57be051dfd49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " File saved: internship_feedback_with_sentiment.csv\n"
     ]
    }
   ],
   "source": [
    "df.to_csv(\"internship_feedback_with_sentiment.csv\", index=False)\n",
    "print(\"\\n File saved: internship_feedback_with_sentiment.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ee4bb13-79dc-4f33-aaf1-0cb15f697dd5",
   "metadata": {},
   "source": [
    "## Accuracy achieved on this dataset: 84.88%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be3b3889-e3bf-4011-9f20-e7dbac235835",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:anaconda3]",
   "language": "python",
   "name": "conda-env-anaconda3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
